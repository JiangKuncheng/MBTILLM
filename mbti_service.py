# -*- coding: utf-8 -*-
"""
MBTIè¯„ä»·æœåŠ¡
"""

import logging
import asyncio
import aiohttp
import json
import re
from typing import Dict, Any, List, Optional
from datetime import datetime
import numpy as np

from new_config import CONFIG
from database_service import db_service
from sohu_client import sohu_client

logger = logging.getLogger(__name__)

class MBTIEvaluationService:
    """MBTIè¯„ä»·æœåŠ¡"""
    
    def __init__(self):
        self.api_key = CONFIG["siliconflow"]["api_key"]
        self.base_url = CONFIG["siliconflow"]["base_url"]
        self.model = CONFIG["siliconflow"]["model"]
        self.timeout = CONFIG["siliconflow"]["timeout"]
        self.max_retries = CONFIG["siliconflow"]["max_retries"]
        self.evaluation_prompt = CONFIG["mbti"]["evaluation_prompt"]
        
        logger.info("MBTIè¯„ä»·æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    async def _call_llm_api(self, content: str) -> Dict[str, Any]:
        """è°ƒç”¨çœŸå®çš„SiliconFlow LLM APIè¿›è¡ŒMBTIè¯„ä»·"""
        import httpx
        import json
        from new_config import SILICONFLOW_CONFIG
        
        try:
            logger.info("è°ƒç”¨çœŸå®çš„SiliconFlow LLM API")
            
            headers = {
                "Authorization": f"Bearer {SILICONFLOW_CONFIG['api_key']}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": SILICONFLOW_CONFIG["model"],
                "messages": [
                    {
                        "role": "user",
                        "content": content
                    }
                ],
                "temperature": 0.3,  # é™ä½éšæœºæ€§ï¼Œæé«˜ä¸€è‡´æ€§
                "max_tokens": 4000,  # å¢åŠ tokenæ•°é‡ï¼Œç¡®ä¿æ‰¹é‡è¯„åˆ†å“åº”å®Œæ•´
                "timeout": SILICONFLOW_CONFIG["timeout"]
            }
            
            async with httpx.AsyncClient(timeout=SILICONFLOW_CONFIG["timeout"]) as client:
                response = await client.post(
                    f"{SILICONFLOW_CONFIG['base_url']}/chat/completions",
                    headers=headers,
                    json=payload
                )
                
                if response.status_code == 200:
                    result = response.json()
                    logger.info("LLM APIè°ƒç”¨æˆåŠŸ")
                    return result
                else:
                    logger.error(f"LLM APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                    # å¦‚æœAPIè°ƒç”¨å¤±è´¥ï¼Œè¿”å›Noneè®©ä¸Šå±‚å¤„ç†
                    return None
                    
        except httpx.TimeoutException:
            logger.error("LLM APIè¯·æ±‚è¶…æ—¶")
            return None
        except httpx.HTTPStatusError as e:
            logger.error(f"LLM API HTTPé”™è¯¯: {e.response.status_code}")
            return None
        except Exception as e:
            logger.error(f"LLM APIè°ƒç”¨å¼‚å¸¸: {e}")
            return None
    
    def _parse_mbti_response(self, llm_response: Dict[str, Any]) -> Dict[str, float]:
        """è§£æLLMå“åº”ä¸­çš„MBTIæ¦‚ç‡"""
        try:
            # æå–æ¶ˆæ¯å†…å®¹
            choices = llm_response.get("choices", [])
            if not choices:
                raise ValueError("LLMå“åº”ä¸­æ²¡æœ‰choices")
            
            content = choices[0].get("message", {}).get("content", "")
            if not content:
                raise ValueError("LLMå“åº”å†…å®¹ä¸ºç©º")
            
            # æŸ¥æ‰¾JSONæ ¼å¼çš„æ¦‚ç‡æ•°æ®
            json_pattern = r'\{[^}]*"[EISNTFJP]"\s*:\s*[0-9.]+[^}]*\}'
            json_matches = re.findall(json_pattern, content)
            
            if json_matches:
                # å°è¯•è§£æç¬¬ä¸€ä¸ªåŒ¹é…çš„JSON
                json_str = json_matches[0]
                try:
                    probabilities = json.loads(json_str)
                    
                    # éªŒè¯æ˜¯å¦åŒ…å«æ‰€æœ‰8ä¸ªç»´åº¦
                    required_traits = ["E", "I", "S", "N", "T", "F", "J", "P"]
                    if all(trait in probabilities for trait in required_traits):
                        # éªŒè¯æ¦‚ç‡å€¼æ˜¯å¦åˆç†
                        for trait in required_traits:
                            prob = probabilities[trait]
                            if not isinstance(prob, (int, float)) or prob < 0 or prob > 1:
                                raise ValueError(f"æ— æ•ˆçš„æ¦‚ç‡å€¼: {trait}={prob}")
                        
                        # å½’ä¸€åŒ–æ¦‚ç‡å¯¹
                        normalized = self._normalize_probabilities(probabilities)
                        logger.info("æˆåŠŸè§£æMBTIæ¦‚ç‡åˆ†å¸ƒ")
                        return normalized
                        
                except json.JSONDecodeError:
                    pass
            
            # å°è¯•å…¶ä»–è§£ææ–¹æ³• - æ”¯æŒå¸¦å¼•å·å’Œä¸å¸¦å¼•å·çš„æ ¼å¼
            prob_pattern = r'([EISNTFJP])\s*:\s*([0-9]*\.?[0-9]+)'
            matches = re.findall(prob_pattern, content)
            
            if len(matches) >= 8:
                probabilities = {}
                for trait, prob_str in matches:
                    try:
                        prob = float(prob_str)
                        if 0 <= prob <= 1:
                            probabilities[trait] = prob
                    except ValueError:
                        continue
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„8ä¸ªç»´åº¦
                required_traits = ["E", "I", "S", "N", "T", "F", "J", "P"]
                if all(trait in probabilities for trait in required_traits):
                    normalized = self._normalize_probabilities(probabilities)
                    logger.info("é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼è§£æMBTIæ¦‚ç‡åˆ†å¸ƒ")
                    return normalized
            
            # å¦‚æœéƒ½æ— æ³•è§£æï¼Œè¿”å›é»˜è®¤å€¼
            logger.warning(f"æ— æ³•è§£æMBTIæ¦‚ç‡ï¼Œä½¿ç”¨é»˜è®¤å€¼ã€‚åŸå§‹å“åº”: {content[:200]}...")
            
        except Exception as e:
            logger.error(f"è§£æMBTIå“åº”å¼‚å¸¸: {e}")
        
        # è¿”å›é»˜è®¤ä¸­æ€§æ¦‚ç‡
        return {"E": 0.5, "I": 0.5, "S": 0.5, "N": 0.5, "T": 0.5, "F": 0.5, "J": 0.5, "P": 0.5}
    
    def _normalize_probabilities(self, probabilities: Dict[str, float]) -> Dict[str, float]:
        """å½’ä¸€åŒ–MBTIæ¦‚ç‡ï¼Œç¡®ä¿æ¯å¯¹å’Œä¸º1.0"""
        normalized = probabilities.copy()
        
        pairs = [("E", "I"), ("S", "N"), ("T", "F"), ("J", "P")]
        for pair in pairs:
            trait1, trait2 = pair
            total = normalized.get(trait1, 0.5) + normalized.get(trait2, 0.5)
            
            if total > 0:
                normalized[trait1] = normalized.get(trait1, 0.5) / total
                normalized[trait2] = normalized.get(trait2, 0.5) / total
            else:
                normalized[trait1] = 0.5
                normalized[trait2] = 0.5
        
        return normalized
    
    def _clean_content(self, content: str) -> str:
        """æ¸…ç†å†…å®¹æ–‡æœ¬"""
        if not content:
            return ""
        
        # ç§»é™¤HTMLæ ‡ç­¾
        content = re.sub(r'<[^>]+>', '', content)
        
        # ç§»é™¤URLé“¾æ¥
        content = re.sub(r'https?://\S+', '', content)
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content)
        
        # é™åˆ¶é•¿åº¦
        if len(content) > 2000:
            content = content[:2000] + "..."
        
        return content.strip()
    
    async def evaluate_content_mbti(self, content: str, content_id: int = None,
                                  content_title: str = None, content_type: str = None) -> Dict[str, float]:
        """è¯„ä»·å†…å®¹çš„MBTIç‰¹å¾"""
        # æ¸…ç†å†…å®¹
        cleaned_content = self._clean_content(content)
        
        if len(cleaned_content) < 10:
            logger.warning("å†…å®¹å¤ªçŸ­ï¼Œä½¿ç”¨é»˜è®¤MBTIæ¦‚ç‡")
            return {"E": 0.5, "I": 0.5, "S": 0.5, "N": 0.5, "T": 0.5, "F": 0.5, "J": 0.5, "P": 0.5}
        
        try:
            # è°ƒç”¨LLM API
            logger.info(f"å¼€å§‹è¯„ä»·å†…å®¹MBTIç‰¹å¾ï¼Œå†…å®¹é•¿åº¦: {len(cleaned_content)}")
            llm_response = await self._call_llm_api(cleaned_content)
            
            # è§£æMBTIæ¦‚ç‡
            probabilities = self._parse_mbti_response(llm_response)
            
            # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¦‚æœæä¾›äº†content_idï¼‰
            if content_id:
                db_service.save_content_mbti(
                    content_id=content_id,
                    probabilities=probabilities,
                    content_title=content_title,
                    content_type=content_type
                )
                logger.info(f"ä¿å­˜å†…å®¹ {content_id} çš„MBTIè¯„ä»·")
            
            return probabilities
            
        except Exception as e:
            logger.error(f"è¯„ä»·å†…å®¹MBTIç‰¹å¾å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å€¼
            return {"E": 0.5, "I": 0.5, "S": 0.5, "N": 0.5, "T": 0.5, "F": 0.5, "J": 0.5, "P": 0.5}
    
    async def evaluate_content_by_id(self, content_id: int, content_type: str = None) -> Dict[str, float]:
        """æ ¹æ®å†…å®¹IDè¯„ä»·MBTIç‰¹å¾"""
        # å…ˆæ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰è¯„ä»·
        existing_mbti = db_service.get_content_mbti(content_id)
        if existing_mbti:
            logger.info(f"å†…å®¹ {content_id} çš„MBTIè¯„ä»·å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›")
            return existing_mbti.to_dict()["probabilities"]
        
        # æ¼”ç¤ºå†…å®¹æ•°æ®ï¼ˆç”¨äºæœç‹APIä¸å¯ç”¨æ—¶ï¼‰
        demo_contents = {
            1001: {
                "title": "å›¢é˜Ÿåä½œçš„åŠ›é‡",
                "content": "æˆ‘çœŸå¿ƒå–œæ¬¢å’Œå›¢é˜Ÿä¸€èµ·å·¥ä½œï¼ä»Šå¤©æˆ‘ä»¬å°ç»„è®¨è®ºäº†æ–°é¡¹ç›®ï¼Œæ¯ä¸ªäººéƒ½ç§¯æå‘è¨€ï¼Œåˆ†äº«è‡ªå·±çš„æƒ³æ³•ã€‚é€šè¿‡å¤§å®¶çš„é›†æ€å¹¿ç›Šï¼Œæˆ‘ä»¬å¾ˆå¿«å°±æ‰¾åˆ°äº†åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚å›¢é˜Ÿåˆä½œè®©æˆ‘æ„Ÿåˆ°å……æ»¡æ´»åŠ›ï¼Œçœ‹åˆ°æ¯ä¸ªäººçš„è´¡çŒ®æ±‡èšæˆæœ€ç»ˆæˆæœçœŸçš„å¾ˆæœ‰æˆå°±æ„Ÿï¼",
                "type": "article"
            },
            1002: {
                "title": "ç‹¬å¤„æ€è€ƒçš„ä»·å€¼",
                "content": "æˆ‘å‘ç°æœ€å¥½çš„æƒ³æ³•å¾€å¾€åœ¨ç‹¬å¤„æ—¶äº§ç”Ÿã€‚ä»Šå¤©ä¸‹åˆä¸€ä¸ªäººåœ¨å’–å•¡å…é‡Œé™é™æ€è€ƒï¼Œçªç„¶å¯¹å¤æ‚çš„æŠ€æœ¯é—®é¢˜æœ‰äº†æ–°çš„ç†è§£ã€‚æˆ‘å–œæ¬¢è¿™ç§æ·±åº¦æ€è€ƒçš„è¿‡ç¨‹ï¼Œèƒ½å¤Ÿå®Œå…¨æ²‰æµ¸åœ¨é—®é¢˜çš„æœ¬è´¨ä¸­ï¼Œä¸è¢«å¤–ç•Œå¹²æ‰°ã€‚ç‹¬ç«‹åˆ†æè®©æˆ‘èƒ½å¤Ÿçœ‹åˆ°åˆ«äººå¿½ç•¥çš„ç»†èŠ‚ã€‚",
                "type": "article"
            },
            1003: {
                "title": "æ•°æ®é©±åŠ¨çš„å†³ç­–",
                "content": "åœ¨åšé‡è¦å†³ç­–æ—¶ï¼Œæˆ‘åšæŒç”¨æ•°æ®è¯´è¯ã€‚é€šè¿‡æ”¶é›†ç›¸å…³æŒ‡æ ‡ã€åˆ†æå†å²è¶‹åŠ¿ã€å»ºç«‹é¢„æµ‹æ¨¡å‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåšå‡ºæ›´åŠ ç†æ€§å’Œå‡†ç¡®çš„åˆ¤æ–­ã€‚æ„Ÿæ€§çš„ç›´è§‰æœ‰æ—¶ä¼šè¯¯å¯¼æˆ‘ä»¬ï¼Œåªæœ‰å®¢è§‚çš„é€»è¾‘åˆ†ææ‰èƒ½ç¡®ä¿å†³ç­–çš„ç§‘å­¦æ€§å’Œæœ‰æ•ˆæ€§ã€‚",
                "type": "article"
            },
            1004: {
                "title": "å…³æ€€ä»–äººçš„é‡è¦æ€§",
                "content": "æˆ‘å§‹ç»ˆç›¸ä¿¡ï¼Œå…³å¿ƒä»–äººçš„æ„Ÿå—æ¯”è¿½æ±‚å®Œç¾çš„é€»è¾‘æ›´é‡è¦ã€‚åœ¨å›¢é˜Ÿä¸­ï¼Œæˆ‘ç»å¸¸ä¼šè¯¢é—®åŒäº‹çš„æƒ³æ³•ï¼Œç¡®ä¿æ¯ä¸ªäººéƒ½æ„Ÿåˆ°è¢«å°Šé‡å’Œç†è§£ã€‚å½“æœ‰äººé‡åˆ°å›°éš¾æ—¶ï¼Œæˆ‘ä¼šä¸»åŠ¨æä¾›å¸®åŠ©å’Œæƒ…æ„Ÿæ”¯æŒã€‚äººä¸äººä¹‹é—´çš„æ¸©æš–è¿æ¥æ˜¯å·¥ä½œå’Œç”Ÿæ´»ä¸­æœ€çè´µçš„è´¢å¯Œã€‚",
                "type": "article"
            },
            1005: {
                "title": "è®¡åˆ’çš„é‡è¦æ€§",
                "content": "æˆ‘æ˜¯ä¸€ä¸ªéå¸¸æœ‰è®¡åˆ’æ€§çš„äººï¼Œå–œæ¬¢æŠŠæ‰€æœ‰äº‹æƒ…éƒ½å®‰æ’å¾—äº•äº•æœ‰æ¡ã€‚æ¯å¤©æ—©ä¸Šæˆ‘éƒ½ä¼šåˆ¶å®šè¯¦ç»†çš„å¾…åŠæ¸…å•ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åºä»»åŠ¡ã€‚è¿™æ ·çš„å®‰æ’è®©æˆ‘èƒ½å¤Ÿé«˜æ•ˆåœ°å®Œæˆå·¥ä½œï¼Œé¿å…é—æ¼é‡è¦äº‹é¡¹ã€‚æˆ‘è®¤ä¸ºè‰¯å¥½çš„è®¡åˆ’æ˜¯æˆåŠŸçš„åŸºç¡€ã€‚",
                "type": "article"
            },
            1006: {
                "title": "æ‹¥æŠ±å˜åŒ–ä¸çµæ´»æ€§",
                "content": "æˆ‘å–œæ¬¢ä¿æŒå¼€æ”¾çš„å¿ƒæ€ï¼Œéšæ—¶å‡†å¤‡åº”å¯¹æ–°çš„æŒ‘æˆ˜å’Œæœºä¼šã€‚å½“è®¡åˆ’å‘ç”Ÿå˜åŒ–æ—¶ï¼Œæˆ‘ä¼šç§¯æè°ƒæ•´ç­–ç•¥ï¼Œå¯»æ‰¾æ–°çš„å¯èƒ½æ€§ã€‚è¿™ç§çµæ´»æ€§è®©æˆ‘èƒ½å¤Ÿåœ¨ä¸ç¡®å®šçš„ç¯å¢ƒä¸­èŒå£®æˆé•¿ï¼ŒæŠŠæ¯ä¸€æ¬¡å˜åŒ–éƒ½çœ‹ä½œæ˜¯å­¦ä¹ å’Œæˆé•¿çš„æœºä¼šã€‚",
                "type": "article"
            }
        }
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ¼”ç¤ºå†…å®¹
        if content_id in demo_contents:
            demo_data = demo_contents[content_id]
            logger.info(f"ä½¿ç”¨æ¼”ç¤ºå†…å®¹ {content_id}: {demo_data['title']}")
            
            # è¯„ä»·MBTIç‰¹å¾
            return await self.evaluate_content_mbti(
                content=demo_data["content"],
                content_id=content_id,
                content_title=demo_data["title"],
                content_type=demo_data["type"]
            )
        
        # å°è¯•ä»SohuGlobal APIè·å–å†…å®¹
        try:
            async with sohu_client as client:
                content_data = await client.get_content_by_id(content_id, content_type)
            
            if content_data.get("code") != 200:
                raise Exception(f"æœç‹APIè¿”å›é”™è¯¯: {content_data}")
            
            # æå–å†…å®¹ä¿¡æ¯
            data = content_data["data"]
            content = data.get("content", "") or data.get("description", "")
            title = data.get("title", "")
            detected_type = data.get("content_type", content_type)
            
            if not content:
                raise Exception("å†…å®¹ä¸ºç©º")
            
            # è¯„ä»·MBTIç‰¹å¾
            return await self.evaluate_content_mbti(
                content=content,
                content_id=content_id,
                content_title=title,
                content_type=detected_type
            )
            
        except Exception as e:
            logger.error(f"è·å–å†…å®¹ {content_id} å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å€¼
            return {"E": 0.5, "I": 0.5, "S": 0.5, "N": 0.5, "T": 0.5, "F": 0.5, "J": 0.5, "P": 0.5}
    
    async def batch_evaluate_contents(self, contents: List[Dict], 
                                    max_concurrent: int = 3) -> Dict[str, Any]:
        """æ‰¹é‡è¯„ä»·å†…å®¹MBTIç‰¹å¾ - ä¸€æ¬¡æœ€å¤šåŒæ—¶è°ƒç”¨ä¸‰æ¬¡å¤§æ¨¡å‹ï¼Œå¾ªç¯å®Œæˆ
        
        Args:
            contents: å†…å®¹åˆ—è¡¨ï¼Œæ¯ä¸ªå†…å®¹åŒ…å« id, title, content ç­‰å­—æ®µ
            max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆæœ€å¤š3ä¸ªï¼‰
            
        Returns:
            åŒ…å« results æ•°ç»„çš„å­—å…¸
        """
        logger.info(f"å¼€å§‹æ‰¹é‡è¯„ä»· {len(contents)} ä¸ªå†…å®¹çš„MBTIç‰¹å¾")
        
        # é™åˆ¶æœ€å¤§å¹¶å‘æ•°ä¸º3
        max_concurrent = min(max_concurrent, 3)
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰MBTIè¯„åˆ†
        contents_to_evaluate = []
        cached_results = []
        
        for content in contents:
            content_id = content.get('id')
            existing_mbti = db_service.get_content_mbti(content_id)
            if existing_mbti:
                cached_results.append({
                    'id': content_id,
                    'title': content.get('title', ''),
                    'E_I': existing_mbti.E,
                    'S_N': existing_mbti.S,
                    'T_F': existing_mbti.T,
                    'J_P': existing_mbti.P,
                    'from_cache': True
                })
            else:
                contents_to_evaluate.append(content)
        
        logger.info(f"ç¼“å­˜å†…å®¹: {len(cached_results)}, éœ€è¦è¯„ä»·: {len(contents_to_evaluate)}")
        
        if not contents_to_evaluate:
            logger.info("æ‰€æœ‰å†…å®¹éƒ½å·²æœ‰MBTIè¯„åˆ†ï¼Œæ— éœ€è°ƒç”¨å¤§æ¨¡å‹")
            return {
                'results': cached_results,
                'total': len(contents),
                'successful': len(contents),
                'cached': len(cached_results),
                'new_evaluated': 0
            }
        
        # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤šmax_concurrentä¸ªå†…å®¹
        batch_size = max_concurrent
        all_results = cached_results.copy()
        
        for i in range(0, len(contents_to_evaluate), batch_size):
            batch = contents_to_evaluate[i:i + batch_size]
            logger.info(f"å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹ï¼ŒåŒ…å« {len(batch)} ä¸ªå†…å®¹")
            
            try:
                # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œæ‰¹é‡è¯„åˆ†
                batch_results = await self._batch_evaluate_with_llm(batch)
                
                if batch_results:
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    for result in batch_results:
                        if result.get('success') and not result.get('from_cache'):
                            content_id = result.get('id')
                            probabilities = {
                                'E': result.get('E_I', 0.5),
                                'I': 1.0 - result.get('E_I', 0.5),
                                'S': result.get('S_N', 0.5),
                                'N': 1.0 - result.get('S_N', 0.5),
                                'T': result.get('T_F', 0.5),
                                'F': 1.0 - result.get('T_F', 0.5),
                                'J': result.get('J_P', 0.5),
                                'P': 1.0 - result.get('J_P', 0.5)
                            }
                            db_service.save_content_mbti(content_id, probabilities)
                            logger.info(f"å†…å®¹ {content_id} MBTIè¯„åˆ†å®Œæˆå¹¶ä¿å­˜")
                    
                    all_results.extend(batch_results)
                else:
                    logger.error(f"ç¬¬ {i//batch_size + 1} æ‰¹è¯„åˆ†å¤±è´¥")
                    # ä½¿ç”¨é»˜è®¤å€¼
                    for content in batch:
                        all_results.append({
                            'id': content.get('id'),
                            'title': content.get('title', ''),
                            'E_I': 0.5,
                            'S_N': 0.5,
                            'T_F': 0.5,
                            'J_P': 0.5,
                            'from_cache': False,
                            'error': 'æ‰¹é‡è¯„åˆ†å¤±è´¥'
                        })
                
                # æ‰¹æ¬¡é—´çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
                if i + batch_size < len(contents_to_evaluate):
                    await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹æ—¶å¼‚å¸¸: {e}")
                # ä½¿ç”¨é»˜è®¤å€¼
                for content in batch:
                    all_results.append({
                        'id': content.get('id'),
                        'title': content.get('title', ''),
                        'E_I': 0.5,
                        'S_N': 0.5,
                        'T_F': 0.5,
                        'J_P': 0.5,
                        'from_cache': False,
                        'error': str(e)
                    })
        
        logger.info(f"æ‰¹é‡è¯„ä»·å®Œæˆ: {len(all_results)}/{len(contents)} ä¸ªå†…å®¹")
        
        return {
            'results': all_results,
            'total': len(contents),
            'successful': len(all_results),
            'cached': len(cached_results),
            'new_evaluated': len(all_results) - len(cached_results)
        }
    
    async def _batch_evaluate_with_llm(self, contents: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨å¤§æ¨¡å‹æ‰¹é‡è¯„ä»·å¤šä¸ªå†…å®¹çš„MBTIç‰¹å¾
        
        Args:
            contents: å†…å®¹åˆ—è¡¨ï¼Œæ¯ä¸ªå†…å®¹åŒ…å« id, title, content ç­‰å­—æ®µ
            
        Returns:
            è¯„ä»·ç»“æœåˆ—è¡¨
        """
        try:
            # æ„å»ºæ‰¹é‡å†…å®¹çš„æç¤ºè¯
            batch_content = self._build_batch_content_for_llm(contents)
            
            # ä½¿ç”¨æ‰¹é‡æç¤ºè¯æ¨¡æ¿
            prompt = CONFIG["mbti"]["batch_evaluation_prompt"].format(
                contents=batch_content
            )
            
            logger.info(f"è°ƒç”¨å¤§æ¨¡å‹æ‰¹é‡è¯„ä»· {len(contents)} ä¸ªå†…å®¹")
            
            # è°ƒç”¨å¤§æ¨¡å‹
            response = await self._call_llm_api(prompt)
            
            if response and response.get('choices'):
                content_text = response['choices'][0].get('message', {}).get('content', '')
                
                # æ— è®ºè§£ææ˜¯å¦å¤±è´¥ï¼Œéƒ½æ‰“å°å®Œæ•´å“åº”
                print(f"\nğŸ” å¤§æ¨¡å‹å®Œæ•´å“åº”å†…å®¹:")
                print("=" * 80)
                print(content_text)
                print("=" * 80)
                
                # è§£ææ‰¹é‡å“åº”
                batch_results = self._parse_batch_llm_response(content_text, contents)
                
                if batch_results:
                    logger.info(f"æ‰¹é‡è¯„ä»·æˆåŠŸ: {len(batch_results)} ä¸ªç»“æœ")
                    return batch_results
                else:
                    logger.warning("æ‰¹é‡å“åº”è§£æå¤±è´¥")
                    return None
            else:
                logger.error(f"å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥: {response}")
                return None
                
        except Exception as e:
            logger.error(f"æ‰¹é‡è¯„ä»·å¼‚å¸¸: {e}")
            return None
    
    def _build_batch_content_for_llm(self, contents: List[Dict]) -> str:
        """æ„å»ºæ‰¹é‡å†…å®¹çš„æç¤ºè¯
        
        Args:
            contents: å†…å®¹åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„æ‰¹é‡å†…å®¹å­—ç¬¦ä¸²
        """
        batch_lines = []
        for i, content in enumerate(contents, 1):
            content_id = content.get('id', 'unknown')
            title = content.get('title', '')[:100]  # é™åˆ¶æ ‡é¢˜é•¿åº¦
            content_text = content.get('content', '')[:500]  # é™åˆ¶å†…å®¹é•¿åº¦
            
            batch_lines.append(f"{i}. ID: {content_id}")
            batch_lines.append(f"   æ ‡é¢˜: {title}")
            batch_lines.append(f"   å†…å®¹: {content_text}")
            batch_lines.append("")  # ç©ºè¡Œåˆ†éš”
        
        return "\n".join(batch_lines)
    
    def _parse_batch_llm_response(self, response_text: str, contents: List[Dict]) -> List[Dict]:
        """è§£æå¤§æ¨¡å‹çš„æ‰¹é‡MBTIè¯„åˆ†å“åº”
        
        Args:
            response_text: å¤§æ¨¡å‹è¿”å›çš„æ–‡æœ¬
            contents: åŸå§‹å†…å®¹åˆ—è¡¨
            
        Returns:
            è¯„ä»·ç»“æœåˆ—è¡¨
        """
        try:
            import json
            import re
            
            # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œæå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                data = json.loads(json_str)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰¹é‡ç»“æœæ ¼å¼
                if 'results' in data and isinstance(data['results'], list):
                    results = []
                    for i, result in enumerate(data['results']):
                        if i < len(contents):  # ç¡®ä¿ä¸è¶…å‡ºåŸå§‹å†…å®¹æ•°é‡
                            content = contents[i]
                            content_id = content.get('id')
                            
                            # æå–MBTIæ•°æ®
                            mbti_data = result.get('mbti_probabilities', result.get('mbti', result))
                            
                            if mbti_data and isinstance(mbti_data, dict):
                                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                                probabilities = {}
                                for trait in ['E', 'I', 'S', 'N', 'T', 'F', 'J', 'P']:
                                    if trait in mbti_data:
                                        probabilities[trait] = float(mbti_data[trait])
                                    else:
                                        probabilities[trait] = 0.5
                                
                                # éªŒè¯æ¦‚ç‡
                                if self._validate_probabilities(probabilities):
                                    results.append({
                                        'id': content_id,
                                        'title': content.get('title', ''),
                                        'E_I': probabilities.get('E', 0.5),
                                        'S_N': probabilities.get('S', 0.5),
                                        'T_F': probabilities.get('T', 0.5),
                                        'J_P': probabilities.get('J', 0.5),
                                        'from_cache': False,
                                        'success': True
                                    })
                                else:
                                    results.append({
                                        'id': content_id,
                                        'title': content.get('title', ''),
                                        'E_I': 0.5,
                                        'S_N': 0.5,
                                        'T_F': 0.5,
                                        'J_P': 0.5,
                                        'from_cache': False,
                                        'success': False,
                                        'error': 'æ¦‚ç‡éªŒè¯å¤±è´¥'
                                    })
                            else:
                                results.append({
                                    'id': content_id,
                                    'title': content.get('title', ''),
                                    'E_I': 0.5,
                                    'S_N': 0.5,
                                    'T_F': 0.5,
                                    'J_P': 0.5,
                                    'from_cache': False,
                                    'success': False,
                                    'error': 'MBTIæ•°æ®æ ¼å¼é”™è¯¯'
                                })
                    
                    return results
            
            logger.warning(f"æ— æ³•è§£ææ‰¹é‡å“åº”: {response_text}")
            return None
            
        except Exception as e:
            logger.error(f"è§£ææ‰¹é‡å“åº”å¼‚å¸¸: {e}")
            return None
    
    async def update_user_mbti_profile(self, user_id: int, force_update: bool = False,
                                     analyze_last_n: int = 100) -> Dict[str, Any]:
        """æ›´æ–°ç”¨æˆ·MBTIæ¡£æ¡ˆ"""
        logger.info(f"å¼€å§‹æ›´æ–°ç”¨æˆ· {user_id} çš„MBTIæ¡£æ¡ˆ")
        
        # è·å–ç”¨æˆ·å½“å‰æ¡£æ¡ˆ
        current_profile = db_service.get_user_profile(user_id)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        if not force_update and current_profile:
            threshold = CONFIG["behavior"]["mbti_update"]["behavior_threshold"]
            if current_profile.behaviors_since_last_update < threshold:
                logger.info(f"ç”¨æˆ· {user_id} è¡Œä¸ºæ•°é‡({current_profile.behaviors_since_last_update})æœªè¾¾åˆ°æ›´æ–°é˜ˆå€¼({threshold})")
                return {
                    "updated": False,
                    "reason": "è¡Œä¸ºæ•°é‡æœªè¾¾åˆ°æ›´æ–°é˜ˆå€¼",
                    "current_profile": current_profile.to_dict() if current_profile else None
                }
        
        # å¦‚æœç”¨æˆ·æ²¡æœ‰MBTIç±»å‹ä½†è¡Œä¸ºæ•°è¾¾åˆ°é˜ˆå€¼ï¼Œå¼ºåˆ¶æ›´æ–°
        if current_profile and not current_profile.mbti_type:
            threshold = CONFIG["behavior"]["mbti_update"]["behavior_threshold"]
            if current_profile.behaviors_since_last_update >= threshold:
                logger.info(f"ç”¨æˆ· {user_id} è¡Œä¸ºæ•°è¾¾åˆ°{threshold}ä½†MBTIæœªå»ºç«‹ï¼Œå¼ºåˆ¶æ›´æ–°")
                force_update = True
        
        # è·å–ç”¨æˆ·æœ€è¿‘çš„è¡Œä¸ºè®°å½•ï¼ˆæ¯æ¬¡æ›´æ–°éƒ½è·å–æœ€æ–°çš„è¡Œä¸ºï¼‰
        recent_behaviors = db_service.get_recent_user_behaviors_for_analysis(user_id, analyze_last_n)
        
        min_behaviors = CONFIG["behavior"]["mbti_update"]["min_behaviors_for_analysis"]
        if len(recent_behaviors) < min_behaviors:
            logger.warning(f"ç”¨æˆ· {user_id} è¡Œä¸ºæ•°é‡({len(recent_behaviors)})ä¸è¶³ï¼Œæœ€å°‘éœ€è¦{min_behaviors}ä¸ªè¡Œä¸º")
            return {
                "updated": False,
                "reason": f"è¡Œä¸ºæ•°é‡ä¸è¶³ï¼Œéœ€è¦è‡³å°‘{min_behaviors}ä¸ªè¡Œä¸º",
                "current_behaviors": len(recent_behaviors)
            }
        
        logger.info(f"ç”¨æˆ· {user_id} æœ‰ {len(recent_behaviors)} ä¸ªè¡Œä¸ºè®°å½•ï¼Œå¼€å§‹åˆ†æ")
        
        # è·å–æ¶‰åŠçš„å†…å®¹IDå’Œæƒé‡
        content_weights = {}
        for behavior in recent_behaviors:
            content_id = behavior.content_id
            weight = behavior.weight
            
            if content_id not in content_weights:
                content_weights[content_id] = 0
            content_weights[content_id] += weight
        
        # è·å–å†…å®¹çš„MBTIè¯„ä»·
        content_ids = list(content_weights.keys())
        logger.info(f"åˆ†æç”¨æˆ· {user_id} çš„ {len(content_ids)} ä¸ªä¸åŒå†…å®¹")
        
        # æ‰¹é‡è·å–æˆ–è¯„ä»·å†…å®¹MBTI
        content_mbti_data = {}
        evaluated_count = 0
        existing_count = 0
        
        for content_id in content_ids:
            existing_mbti = db_service.get_content_mbti(content_id)
            if existing_mbti:
                content_mbti_data[content_id] = existing_mbti.to_dict()["probabilities"]
                existing_count += 1
            else:
                # å¦‚æœæ²¡æœ‰è¯„ä»·ï¼Œå…ˆè¯„ä»·è¿™ä¸ªå†…å®¹
                try:
                    logger.info(f"å†…å®¹ {content_id} æ²¡æœ‰MBTIè¯„åˆ†ï¼Œå¼€å§‹è¯„åˆ†...")
                    probabilities = await self.evaluate_content_by_id(content_id)
                    content_mbti_data[content_id] = probabilities
                    evaluated_count += 1
                    logger.info(f"å†…å®¹ {content_id} MBTIè¯„åˆ†å®Œæˆ")
                except Exception as e:
                    logger.error(f"è¯„ä»·å†…å®¹ {content_id} å¤±è´¥: {e}")
                    # ä½¿ç”¨é»˜è®¤å€¼
                    content_mbti_data[content_id] = {"E": 0.5, "I": 0.5, "S": 0.5, "N": 0.5, 
                                                   "T": 0.5, "F": 0.5, "J": 0.5, "P": 0.5}
        
        logger.info(f"å†…å®¹MBTIåˆ†æå®Œæˆ: {existing_count} ä¸ªå·²æœ‰è¯„åˆ†, {evaluated_count} ä¸ªæ–°è¯„åˆ†")
        
        # è®¡ç®—åŠ æƒå¹³å‡MBTIæ¦‚ç‡
        weighted_probs = {"E": 0, "I": 0, "S": 0, "N": 0, "T": 0, "F": 0, "J": 0, "P": 0}
        total_weight = 0
        
        for content_id, weight in content_weights.items():
            content_probs = content_mbti_data.get(content_id, {})
            for trait in weighted_probs:
                weighted_probs[trait] += content_probs.get(trait, 0.5) * weight
            total_weight += weight
        
        # å½’ä¸€åŒ–
        if total_weight > 0:
            for trait in weighted_probs:
                weighted_probs[trait] /= total_weight
        
        # ä¸å†å²MBTIè¯„åˆ†èåˆ
        final_probs = weighted_probs.copy()
        if current_profile and current_profile.mbti_type:
            # å¦‚æœç”¨æˆ·å·²æœ‰MBTIç±»å‹ï¼Œæ–°æ—§MBTIåšå¹³å‡
            logger.info(f"ç”¨æˆ· {user_id} å·²æœ‰MBTIç±»å‹ {current_profile.mbti_type}ï¼Œè¿›è¡Œæ–°æ—§MBTIå¹³å‡")
            
            current_probs = {
                "E": current_profile.E, "I": current_profile.I,
                "S": current_profile.S, "N": current_profile.N,
                "T": current_profile.T, "F": current_profile.F,
                "J": current_profile.J, "P": current_profile.P
            }
            
            # æ–°æ—§MBTIå„å 50%æƒé‡
            for trait in final_probs:
                final_probs[trait] = (current_probs.get(trait, 0.5) + weighted_probs[trait]) / 2
        else:
            # å¦‚æœç”¨æˆ·æ²¡æœ‰MBTIç±»å‹ï¼Œç›´æ¥ä½¿ç”¨æ–°è®¡ç®—çš„MBTI
            logger.info(f"ç”¨æˆ· {user_id} æ²¡æœ‰MBTIç±»å‹ï¼Œä½¿ç”¨æ–°è®¡ç®—çš„MBTI")
            final_probs = weighted_probs.copy()
        
        # å½’ä¸€åŒ–æœ€ç»ˆæ¦‚ç‡
        final_probs = self._normalize_probabilities(final_probs)
        
        # æ›´æ–°æ•°æ®åº“
        updated_profile = db_service.update_user_profile(
            user_id=user_id,
            probabilities=final_probs,
            total_behaviors=len(recent_behaviors)
        )
        
        logger.info(f"ç”¨æˆ· {user_id} MBTIæ¡£æ¡ˆæ›´æ–°å®Œæˆ: {updated_profile.mbti_type}")
        
        return {
            "updated": True,
            "user_id": user_id,
            "old_mbti_type": current_profile.mbti_type if current_profile else None,
            "new_mbti_type": updated_profile.mbti_type,
            "behaviors_analyzed": len(recent_behaviors),
            "contents_analyzed": len(content_ids),
            "probability_changes": self._calculate_probability_changes(current_profile, final_probs) if current_profile else None,
            "update_time": updated_profile.last_updated.isoformat(),
            "new_profile": updated_profile.to_dict()
        }
    
    def _calculate_probability_changes(self, old_profile, new_probs: Dict[str, float]) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—æ¦‚ç‡å˜åŒ–"""
        if not old_profile:
            return {}
        
        old_probs = {
            "E": old_profile.E, "I": old_profile.I,
            "S": old_profile.S, "N": old_profile.N,
            "T": old_profile.T, "F": old_profile.F,
            "J": old_profile.J, "P": old_profile.P
        }
        
        changes = {}
        for trait in new_probs:
            old_val = old_probs.get(trait, 0.5)
            new_val = new_probs[trait]
            changes[trait] = {
                "old": round(old_val, 3),
                "new": round(new_val, 3),
                "change": round(new_val - old_val, 3)
            }
        
        return changes
    
    async def _evaluate_content_with_llm(self, title: str, content: str) -> Optional[Dict[str, float]]:
        """ä½¿ç”¨LLMè¯„ä»·å•ä¸ªå†…å®¹çš„MBTIç‰¹å¾
        
        Args:
            title: å†…å®¹æ ‡é¢˜
            content: å†…å®¹æ–‡æœ¬
            
        Returns:
            MBTIæ¦‚ç‡å­—å…¸ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            # æ„å»ºå•ä¸ªå†…å®¹çš„æç¤ºè¯
            single_content = f"ID: {title}\næ ‡é¢˜: {title}\nå†…å®¹: {content[:1000]}"
            
            # ä½¿ç”¨æ‰¹é‡æç¤ºè¯æ¨¡æ¿ï¼Œä½†åªä¼ å…¥ä¸€ä¸ªå†…å®¹
            prompt = CONFIG["mbti"]["batch_evaluation_prompt"].format(
                contents=single_content
            )
            
            logger.info(f"è°ƒç”¨LLMè¯„ä»·å†…å®¹: {title[:30]}...")
            
            # è°ƒç”¨LLM
            response = await self._call_llm_api(prompt)
            
            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            print(f"ğŸ” DEBUG: LLMåŸå§‹å“åº”: {response}")
            
            if response and response.get('choices'):
                content_text = response['choices'][0].get('message', {}).get('content', '')
                print(f"ğŸ” DEBUG: LLMå†…å®¹æ–‡æœ¬: {content_text}")
                
                # è§£æLLMå“åº”
                probabilities = self._parse_llm_response(content_text)
                print(f"ğŸ” DEBUG: è§£æåçš„æ¦‚ç‡: {probabilities}")
                
                if probabilities:
                    logger.info(f"LLMè¯„ä»·æˆåŠŸ: {probabilities}")
                    return probabilities
                else:
                    logger.warning(f"LLMå“åº”è§£æå¤±è´¥: {content_text}")
                    return None
            else:
                logger.error(f"LLMè°ƒç”¨å¤±è´¥æˆ–å“åº”æ ¼å¼é”™è¯¯: {response}")
                return None
                
        except Exception as e:
            logger.error(f"LLMè¯„ä»·å¼‚å¸¸: {e}")
            return None
    
    async def ensure_content_mbti_evaluated(self, content_id: int) -> bool:
        """ç¡®ä¿å†…å®¹å·²è¿›è¡ŒMBTIè¯„åˆ†ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿›è¡Œè¯„åˆ†"""
        try:
            # æ£€æŸ¥å†…å®¹æ˜¯å¦å·²ç»æœ‰MBTIè¯„åˆ†
            existing_mbti = db_service.get_content_mbti(content_id)
            if existing_mbti:
                logger.info(f"å†…å®¹ {content_id} å·²æœ‰MBTIè¯„åˆ†ï¼Œè·³è¿‡")
                return True
            
            # å¦‚æœæ²¡æœ‰è¯„åˆ†ï¼Œè¿›è¡Œè¯„åˆ†
            logger.info(f"å†…å®¹ {content_id} æ²¡æœ‰MBTIè¯„åˆ†ï¼Œå¼€å§‹è¯„åˆ†")
            probabilities = await self.evaluate_content_by_id(content_id)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            db_service.save_content_mbti(content_id, probabilities)
            logger.info(f"å†…å®¹ {content_id} MBTIè¯„åˆ†å®Œæˆå¹¶ä¿å­˜")
            return True
            
        except Exception as e:
            logger.error(f"ç¡®ä¿å†…å®¹ {content_id} MBTIè¯„åˆ†å¤±è´¥: {e}")
            return False
    
    def _parse_llm_response(self, response_text: str) -> Optional[Dict[str, float]]:
        """è§£æLLMçš„MBTIè¯„åˆ†å“åº”
        
        Args:
            response_text: LLMè¿”å›çš„æ–‡æœ¬
            
        Returns:
            MBTIæ¦‚ç‡å­—å…¸ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            import json
            import re
            
            # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œæå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                data = json.loads(json_str)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰¹é‡ç»“æœæ ¼å¼
                if 'results' in data and isinstance(data['results'], list) and len(data['results']) > 0:
                    # å–ç¬¬ä¸€ä¸ªç»“æœ
                    first_result = data['results'][0]
                    if 'mbti' in first_result:
                        mbti_data = first_result['mbti']
                    elif 'mbti_probabilities' in first_result:
                        mbti_data = first_result['mbti_probabilities']
                    else:
                        # ç›´æ¥åŒ…å«MBTIæ•°æ®
                        mbti_data = first_result
                    
                    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                    probabilities = {}
                    for trait in ['E', 'I', 'S', 'N', 'T', 'F', 'J', 'P']:
                        if trait in mbti_data:
                            probabilities[trait] = float(mbti_data[trait])
                        else:
                            probabilities[trait] = 0.5
                    
                    # éªŒè¯æ¦‚ç‡
                    if self._validate_probabilities(probabilities):
                        return probabilities
                
                # å¦‚æœä¸æ˜¯æ‰¹é‡æ ¼å¼ï¼Œå°è¯•ç›´æ¥è§£æ
                elif all(trait in data for trait in ['E', 'I', 'S', 'N', 'T', 'F', 'J', 'P']):
                    probabilities = {trait: float(data[trait]) for trait in ['E', 'I', 'S', 'N', 'T', 'F', 'J', 'P']}
                    if self._validate_probabilities(probabilities):
                        return probabilities
            
            logger.warning(f"æ— æ³•è§£æLLMå“åº”: {response_text}")
            return None
            
        except Exception as e:
            logger.error(f"è§£æLLMå“åº”å¼‚å¸¸: {e}")
            return None
    
    def _validate_probabilities(self, probabilities: Dict[str, float]) -> bool:
        """éªŒè¯MBTIæ¦‚ç‡çš„æœ‰æ•ˆæ€§
        
        Args:
            probabilities: MBTIæ¦‚ç‡å­—å…¸
            
        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€çš„ç»´åº¦
            required_traits = ['E', 'I', 'S', 'N', 'T', 'F', 'J', 'P']
            if not all(trait in probabilities for trait in required_traits):
                return False
            
            # æ£€æŸ¥æ¦‚ç‡å€¼æ˜¯å¦åœ¨0-1ä¹‹é—´
            for trait in required_traits:
                if not (0 <= probabilities[trait] <= 1):
                    return False
            
            # æ£€æŸ¥å¯¹ç«‹ç»´åº¦æ¦‚ç‡å’Œæ˜¯å¦ä¸º1
            pairs = [('E', 'I'), ('S', 'N'), ('T', 'F'), ('J', 'P')]
            for trait1, trait2 in pairs:
                if abs(probabilities[trait1] + probabilities[trait2] - 1.0) > 0.01:
                    return False
            
            return True
            
        except Exception as e:
            logger.error(f"éªŒè¯æ¦‚ç‡å¼‚å¸¸: {e}")
            return False
    
    async def update_content_mbti_when_users_reach_50(self, content_id: int, force_update: bool = False) -> Dict[str, Any]:
        """å½“å¸–å­çš„æ“ä½œç”¨æˆ·æ•°é‡è¾¾åˆ°50ä¸ªæ—¶ï¼Œæ›´æ–°å¸–å­çš„MBTI
        
        Args:
            content_id: å†…å®¹ID
            force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
            
        Returns:
            æ›´æ–°ç»“æœå­—å…¸
        """
        logger.info(f"æ£€æŸ¥å†…å®¹ {content_id} æ˜¯å¦éœ€è¦æ›´æ–°MBTIï¼ˆæ“ä½œç”¨æˆ·æ•°é‡è¾¾åˆ°50ï¼‰")
        
        try:
            # è·å–å†…å®¹çš„æ“ä½œç”¨æˆ·æ•°é‡
            operation_users = db_service.get_content_operation_users(content_id)
            user_count = len(operation_users)
            
            # æ£€æŸ¥æ“ä½œç”¨æˆ·æ•°é‡æ˜¯å¦è¾¾åˆ°50
            if not force_update and user_count < 50:
                logger.info(f"å†…å®¹ {content_id} æ“ä½œç”¨æˆ·æ•°é‡({user_count})æœªè¾¾åˆ°50ï¼Œè·³è¿‡æ›´æ–°")
                return {
                    "updated": False,
                    "reason": f"æ“ä½œç”¨æˆ·æ•°é‡({user_count})æœªè¾¾åˆ°50",
                    "content_id": content_id
                }
            
            logger.info(f"å†…å®¹ {content_id} æ“ä½œç”¨æˆ·æ•°é‡è¾¾åˆ°{user_count}ï¼Œå¼€å§‹æ›´æ–°MBTI")
            
            # æ­¥éª¤1: è®¡ç®—æ“ä½œç”¨æˆ·çš„å¹³å‡MBTI
            user_mbti_list = []
            for user_id in operation_users:
                user_profile = db_service.get_user_profile(user_id)
                if user_profile and user_profile.mbti_type:
                    user_mbti = {
                        "E": user_profile.E, "I": user_profile.I,
                        "S": user_profile.S, "N": user_profile.N,
                        "T": user_profile.T, "F": user_profile.F,
                        "J": user_profile.J, "P": user_profile.P
                    }
                    user_mbti_list.append(user_mbti)
            
            if not user_mbti_list:
                logger.warning(f"å†…å®¹ {content_id} çš„æ“ä½œç”¨æˆ·éƒ½æ²¡æœ‰MBTIæ¡£æ¡ˆ")
                return {
                    "updated": False,
                    "reason": "æ“ä½œç”¨æˆ·éƒ½æ²¡æœ‰MBTIæ¡£æ¡ˆ",
                    "content_id": content_id
                }
            
            # è®¡ç®—ç”¨æˆ·å¹³å‡MBTI
            avg_user_mbti = self._calculate_average_mbti(user_mbti_list)
            logger.info(f"æ“ä½œç”¨æˆ·å¹³å‡MBTI: {avg_user_mbti}")
            
            # æ­¥éª¤2: è·å–å†…å®¹å½“å‰çš„MBTIè¯„åˆ†
            current_content_mbti = db_service.get_content_mbti(content_id)
            if not current_content_mbti:
                logger.warning(f"å†…å®¹ {content_id} æ²¡æœ‰å½“å‰MBTIè¯„åˆ†")
                return {
                    "updated": False,
                    "reason": "å†…å®¹æ²¡æœ‰å½“å‰MBTIè¯„åˆ†",
                    "content_id": content_id
                }
            
            current_content_probs = {
                "E": current_content_mbti.E, "I": current_content_mbti.I,
                "S": current_content_mbti.S, "N": current_content_mbti.N,
                "T": current_content_mbti.T, "F": current_content_mbti.F,
                "J": current_content_mbti.J, "P": current_content_mbti.P
            }
            
            # æ­¥éª¤3: è®¡ç®—æ–°çš„å†…å®¹MBTIï¼ˆç”¨æˆ·å¹³å‡MBTIå’Œå†…å®¹æ—§MBTIçš„å¹³å‡å€¼ï¼‰
            new_content_mbti = {}
            for trait in ["E", "I", "S", "N", "T", "F", "J", "P"]:
                user_avg = avg_user_mbti.get(trait, 0.5)
                content_old = current_content_probs.get(trait, 0.5)
                # æ–°æ—§å„å 50%æƒé‡
                new_content_mbti[trait] = (user_avg + content_old) / 2
            
            # å½’ä¸€åŒ–æ–°å†…å®¹MBTI
            new_content_mbti = self._normalize_probabilities(new_content_mbti)
            
            # æ­¥éª¤4: æ›´æ–°å†…å®¹MBTI
            updated_content = db_service.update_content_mbti(
                content_id=content_id,
                probabilities=new_content_mbti
            )
            
            logger.info(f"å†…å®¹ {content_id} MBTIæ›´æ–°å®Œæˆ")
            
            return {
                "updated": True,
                "content_id": content_id,
                "user_count": user_count,
                "old_content_mbti": current_content_probs,
                "new_content_mbti": new_content_mbti,
                "avg_user_mbti": avg_user_mbti,
                "total_users": len(operation_users),
                "update_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"æ›´æ–°å†…å®¹ {content_id} MBTIå¤±è´¥: {e}")
            return {
                "updated": False,
                "reason": str(e),
                "content_id": content_id
            }
    
    async def update_user_mbti_when_posts_reach_50_multiple(self, user_id: int, force_update: bool = False) -> Dict[str, Any]:
        """å½“ç”¨æˆ·æ“ä½œçš„å¸–å­æ•°é‡è¾¾åˆ°50å€æ•°æ—¶ï¼Œæ›´æ–°ç”¨æˆ·çš„MBTI
        
        Args:
            user_id: ç”¨æˆ·ID
            force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
            
        Returns:
            æ›´æ–°ç»“æœå­—å…¸
        """
        logger.info(f"æ£€æŸ¥ç”¨æˆ· {user_id} æ˜¯å¦éœ€è¦æ›´æ–°MBTIï¼ˆæ“ä½œå¸–å­æ•°é‡è¾¾åˆ°50å€æ•°ï¼‰")
        
        try:
            # è·å–ç”¨æˆ·æ“ä½œçš„å¸–å­æ•°é‡
            user_operation_posts = db_service.get_user_operation_posts(user_id)
            post_count = len(user_operation_posts)
            
            # æ£€æŸ¥å¸–å­æ•°é‡æ˜¯å¦è¾¾åˆ°50çš„å€æ•°
            if not force_update and post_count % 50 != 0:
                logger.info(f"ç”¨æˆ· {user_id} æ“ä½œå¸–å­æ•°é‡({post_count})æœªè¾¾åˆ°50çš„å€æ•°ï¼Œè·³è¿‡æ›´æ–°")
                return {
                    "updated": False,
                    "reason": f"æ“ä½œå¸–å­æ•°é‡({post_count})æœªè¾¾åˆ°50çš„å€æ•°",
                    "current_posts": post_count
                }
            
            logger.info(f"ç”¨æˆ· {user_id} æ“ä½œå¸–å­æ•°é‡è¾¾åˆ°{post_count}ï¼ˆ50çš„å€æ•°ï¼‰ï¼Œå¼€å§‹æ›´æ–°MBTI")
            
            # æ­¥éª¤1: è·å–ç”¨æˆ·å½“å‰MBTIæ¡£æ¡ˆ
            current_profile = db_service.get_user_profile(user_id)
            if not current_profile:
                logger.warning(f"ç”¨æˆ· {user_id} æ²¡æœ‰MBTIæ¡£æ¡ˆ")
                return {
                    "updated": False,
                    "reason": "ç”¨æˆ·æ²¡æœ‰MBTIæ¡£æ¡ˆ",
                    "user_id": user_id
                }
            
            current_user_probs = {
                "E": current_profile.E, "I": current_profile.I,
                "S": current_profile.S, "N": current_profile.N,
                "T": current_profile.T, "F": current_profile.F,
                "J": current_profile.J, "P": current_profile.P
            }
            
            # æ­¥éª¤2: è®¡ç®—ç”¨æˆ·æ“ä½œå¸–å­çš„å¹³å‡MBTI
            post_mbti_list = []
            for post_id in user_operation_posts:
                post_mbti = db_service.get_content_mbti(post_id)
                if post_mbti:
                    post_mbti_data = {
                        "E": post_mbti.E, "I": post_mbti.I,
                        "S": post_mbti.S, "N": post_mbti.N,
                        "T": post_mbti.T, "F": post_mbti.F,
                        "J": post_mbti.J, "P": post_mbti.P
                    }
                    post_mbti_list.append(post_mbti_data)
            
            if not post_mbti_list:
                logger.warning(f"ç”¨æˆ· {user_id} æ“ä½œçš„å¸–å­éƒ½æ²¡æœ‰MBTIè¯„åˆ†")
                return {
                    "updated": False,
                    "reason": "æ“ä½œçš„å¸–å­éƒ½æ²¡æœ‰MBTIè¯„åˆ†",
                    "user_id": user_id
                }
            
            # è®¡ç®—å¸–å­å¹³å‡MBTI
            avg_post_mbti = self._calculate_average_mbti(post_mbti_list)
            logger.info(f"ç”¨æˆ·æ“ä½œå¸–å­å¹³å‡MBTI: {avg_post_mbti}")
            
            # æ­¥éª¤3: è®¡ç®—æ–°çš„ç”¨æˆ·MBTIï¼ˆå½“å‰ç”¨æˆ·MBTIå’Œå¸–å­å¹³å‡MBTIçš„å¹³å‡å€¼ï¼‰
            new_user_mbti = {}
            for trait in ["E", "I", "S", "N", "T", "F", "J", "P"]:
                user_current = current_user_probs.get(trait, 0.5)
                post_avg = avg_post_mbti.get(trait, 0.5)
                # æ–°æ—§å„å 50%æƒé‡
                new_user_mbti[trait] = (user_current + post_avg) / 2
            
            # å½’ä¸€åŒ–æ–°ç”¨æˆ·MBTI
            new_user_mbti = self._normalize_probabilities(new_user_mbti)
            
            # æ­¥éª¤4: æ›´æ–°ç”¨æˆ·MBTIæ¡£æ¡ˆ
            updated_profile = db_service.update_user_profile(
                user_id=user_id,
                probabilities=new_user_mbti,
                total_behaviors=current_profile.behaviors_since_last_update
            )
            
            logger.info(f"ç”¨æˆ· {user_id} MBTIæ›´æ–°å®Œæˆ: {updated_profile.mbti_type}")
            
            return {
                "updated": True,
                "user_id": user_id,
                "post_count": post_count,
                "old_mbti_type": current_profile.mbti_type,
                "new_mbti_type": updated_profile.mbti_type,
                "old_probabilities": current_user_probs,
                "new_probabilities": new_user_mbti,
                "avg_post_mbti": avg_post_mbti,
                "update_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"æ›´æ–°ç”¨æˆ· {user_id} MBTIå¤±è´¥: {e}")
            return {
                "updated": False,
                "reason": str(e),
                "user_id": user_id
            }
    
    def _calculate_average_mbti(self, mbti_list: List[Dict[str, float]]) -> Dict[str, float]:
        """è®¡ç®—å¤šä¸ªMBTIæ¦‚ç‡çš„å¹³å‡å€¼"""
        if not mbti_list:
            return {"E": 0.5, "I": 0.5, "S": 0.5, "N": 0.5, 
                   "T": 0.5, "F": 0.5, "J": 0.5, "P": 0.5}
        
        avg_mbti = {"E": 0, "I": 0, "S": 0, "N": 0, "T": 0, "F": 0, "J": 0, "P": 0}
        
        for mbti in mbti_list:
            for trait in avg_mbti:
                avg_mbti[trait] += mbti.get(trait, 0.5)
        
        # è®¡ç®—å¹³å‡å€¼
        count = len(mbti_list)
        for trait in avg_mbti:
            avg_mbti[trait] /= count
        
        # å½’ä¸€åŒ–
        return self._normalize_probabilities(avg_mbti)

# å…¨å±€MBTIè¯„ä»·æœåŠ¡å®ä¾‹
mbti_service = MBTIEvaluationService()
